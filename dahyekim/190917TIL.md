# 190917 통계수업
## 잔차(Residual) - 예측이 벗어나는 것
* 회귀 분석의 예측과 실제값의 차이
* 잔차의 분포에서 주목하는 특성들
    * 왜도(Skewness) - 데이터가 어느 한쪽으로 치우쳤다.
        * Negative Skew
        * Positive Skew - ex)소득.. 빌게이츠의 여름휴가처럼....
    * 첨도(Kurtosis)
        * 분포가 한 점에 몰린 정도
        * 정규분포의 첨도 = 3
        * 첨도가 높다 -> 데이터가 중심에 몰려 있음
        * 첨도가 낮다 -> 데이터가 바깥으로 퍼져 있음
    * 잔차의 정규성(Normality)
        * 잔차가 정규분포에 가까운 성질을 가지고 있는가
    * <b>등분산성(Homoscedasticity)</b> **중요
        * 모든 범위에서 잔차의 분산이 같음
        * 쉽게 말해 어떤 x에서든 비슷한 정도로 y를 맞출 수 있음
        * Dublin-Watson 통계량이 1~2 정도 되면은 적당하다고 판단
    * 조건수(Condition Number) - 입력에 얼마나 민감한지
        * 입력의 변화에 따른 출력의 변화를 나타내는 수
        * 조건수가 크면 -> 데이터가 조금만 달라져도 결과에 큰 차이
        * 조건수가 작으면 -> 아무리 뭘해도 변화가 거의 없음
        * 통상 30이하가 적당
    * <b>다중공선성(Multicollinearity)</b> **중요
        * 여러 독립변수들이 서로 예측가능할 경우
        * 조건수가 커짐
        * 데이터나 변수의 변화에 따라 추정된 계수가 크게 달라짐
        * 다중공선성이 있으면 계수 추정이 불안정해지고 조건수가 높아진다.
## 변수선택(Variable Selection)
*  K-fold Cross Validation
    * 데이터를 k개로 나누어 CV를 k번 하는 방법
    * ex) k=3일 경우 
        * 1, 2번 데이터로 추정 -> 3번 데이터로 검증
        * 2, 3번 데이터로 추정 -> 1번 데이터로 검증
        * 1, 3번 데이터로 추정 -> 2번 데이터로 검증
* 정규화(Regularization)
    * 과적합(Overfitting) : 모형의 계수가 주어진 데이터(샘플)에 지나치게 의존하여 추정되는 경우
    * 변수가 많을수룩, 계수가 클수록 과적합의 위험이 큼(변수가 10개면 경우의 수가 2^10개)
        * 동일한 상황이면 변수가 적은게 좋다.
        * 계수가 작다 -> 기울기가 작다 -> underfit(<->과적합)
    * 만약 계수가 0이라면 변수가 없는거랑 똑같음
    * 정규화 : 가능한 계수를 작게 추정하는 방법 - 과적합을 막아줌
    * 오차만 작아야 하는게 아니라 계수도 작게 만들어야 한다
        * 왜 계수를 작게해? 과적합을 막으려고. 왜 막아? 예측을 잘하려고.

복잡한 모형 : 곡선형태, 변수가 많은 것
* 라쏘 회귀분석(Lasso)
    * 계수의 절대값을 최소화
        * MSE + λΣ|w|
    * λ(람다) : 클 수록 계수를 최소화하는데 더 큰 비중
    * 회귀계수를 0으로 만드는 경향 -> 변수선택
* 릿지 회귀분석(Ridge) - 라쏘보다 예측이 더 잘됨. 라쏘가 해석은 더 심플함
    * 계수의 제곱을 최소화
        * MSE + λΣw^2
    * 라쏘보다 더 나음.
* 엘라스틱 넷(Elastic Net) - 짬짜면
    * 라쏘 + 릿지
        * MSE + λ(aΣ|w|+(1-a)Σw^2)
        * a=1 : 라쏘(L1)
        * a=0 : 릿지(L2)
* 하이퍼파라미터(Hyperparameter)
    * 모형의 특성을 결정하지만 데이터로부터 학습되지 않는 값
    * 엘라스틱 넷에서 λ와 a
    * CV를 통해 결정

## 고급 회귀분석
* 더미 코딩(Dummy coding)
    * 독립변수에 이산형(범주형) 변수가 있을 경우
        * ex) 짜장, 짬뽕, 볶음밥
    * 기준이 되는 값을 정함
        * ex) 짜장
    * 나머지 값을 새로운 변수로 추가
        * ex) 짬뽕, 볶음밥
    * 해당되는 변수의 값을 1, 나머지는 0으로 설정
* 상호작용(Interaction)
    * 두 독립변수의 곱으로 이뤄진 항(xm)
        * y = x + m + xm
    * 간단히 하기 위해 m을 0 또는 1만 갖는 범주형 변수라고 하면 (아니어도 됨)
    * 상호작용이 없는 경우
        * y = x + m -> m에 따라 x의 절편이 바뀌는 것으로 해석
    * 상호작용이 있는 경우(1)
        * y = x + xm -> m에 따라 x의 기울기가 바뀌는 것으로 해석
    * 상호작용이 있는 경우(2)
        * y = x + m + xm -> m에 따라 x의 절편과 기울기가 바뀌는 것으로 해석

## 문제풀이(데잇걸즈 복습7)
1. 선형모형 vs 로지스틱 선형모형
    * 선형모형은 연속
    * 로지스틱은 이거냐 저거냐
2. 신뢰구간이 넓을 때
    * 데이터를 많이 모아야 신뢰구간이 좁아진다.
    * 더 많은 실험을 해서 샘플의 크기를 키운다.
    * 리샘플링은 많이 하면 할수록 좋다.
3. 두 집단의 평균 차이를 검정하기 위해 부트스트래핑 할 때는 두 집단이 원래 같다고 가정한다.
4. 두 집단의 평균 차이를 검정하기 위해 부트스트래핑 할 때는 집단을 무시하고 데이터를 섞은 후, 2개의 샘플을 리샘플링으로 뽑는다.
5. 두 집단의 평균 차이를 검정하기 위해 부트스트래핑 할 때는 2개의 샘플에서 평균 차이를 구하여 그 신뢰구간을 구한다.
6. 두 집단이 다르다고 결론을 내리려면, 실험을 통해 관찰된 두 집단의 차이가 부트스트래핑을 통해 알아낸 신뢰구간의 범위 밖으로 벗어나야 한다.
7. 상관계수
    * 두 변수가 얼마나 관련이 있는지
    * U자형 관계는 관계는 있지만 상관계수는 0
    * 범위는 -1에서 1까지
    * 한 변수가 증가할 때 다른 변수도 증가하면 +
    * 한 변수가 증가할 때 다른 변수는 감소하면 -
    * 서열의 상관을 구할 때는 스피어만 상관계수나 켄달 상관계수를 사용한다
8. 선형 회귀분석을 실시한 결과 R제곱(R-squared)가 0.8이 나왔습니다. 분석 모형은 종속변수의 분산의 몇 %를 설명합니까?
    * 80%
9. 선형 회귀분석의 모형을 비교할 때 사용하는 적합도 지수
    * Adj. R-squared
    * AIC
    * BIC
10. 선형 회귀분석에서 회귀계수의 신뢰구간
    * 플러스든 마이너스든 어느 한쪽에만 있어야 한다