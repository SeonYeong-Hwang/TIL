# 자연어처리 특강 이기창님2019-11-11

네이버 클로바 챗봇 모델

# 임베딩

자연어처리의 핵심

단어나 문장을 벡터(숫자)로 바꾼 것 혹은 그 과정

1. 단어의 빈도 세기 Tool: 은전한닢

    단어 임베딩: 각 문서에 해당 단어가 몇 번 들어가 있는지, 

    문서 임베딩: 문서에 각 단어가 몇 번 들어가 있는지, 

    품질이 좋지 않다

2. Word2Vec 임베딩 기법

500기가의 한국어 말뭉치를 가지고 학습해서 만든 임베딩

벡터로 바꾼 순간 계산이 가능해진다

sim 계산

임베딩으로 할 수 있는 것

관련도/유사도 계산: 히트맵, scatterplot, 도표

벡터 연산(유추 평가): 아들 - 딸 + 소녀 = 소년

단어 임베딩으로 문서 분류하기(1)

핵심컨셉: 문서에 속한 단어가 유사하면 문서 의미도 비슷하다

문서 벡터를 만드는 방법: 단어 임베딩의 합으로 문서 벡터를 표현한다

임베딩 품질이 좋으면 자연어처리 품질이 좋아진다

label 을 노가다로 만들어야 한다

`e.g.` 감정(긍정/부정)

단어 임베딩으로 문서 분류하기(2)

핵심 컨셉: 유의어가 많이 포함된 문장을 자동으로 추출한다

비슷한 의미의 단어는 가까이에 모여있다

- 유클리디안 거리 행렬 구축(피타고라스 정리)

    문제점: 쿼리 단어들마다 거리가 들쭉날쭉하다 → 정규화 Normalization 0~1

- 가중치 행렬로 문장별 스코어 구하기

    문장 내 각 단어가 타겟 단어(`e.g.` `배터리`)와 얼마나 관련있는지 나타낸 스코어를 모두 더한다

- 각 단어의 배터리와 상관 스코어 행렬을 Document-Term Matrix (문장 내 해당 단어가 들어있으면 1, 없으면 0) 와 내적

    시간을 더 줄인다

label 필요 없다, 대신 정확도 낮다

`e.g.` `배터리`

밧데리 효율, 조루 베터리, 발열

→ 오타도 커버

임베딩이 어떤 의미를 가지는가

각 임베딩 기법이 왜 잘 되는지는 아직 연구중

임베딩 안에 말뭉치의 통계적 패턴 정보가 들어 있다

빈도 정보: 저자의 의도는 단어 사용 패턴에 드러난다

Bag of Words 가정

순서 정보: 단어가 어떤 순서로 나타나는지 살핀다

ELMo, GPT 일방향 순서 확인

BERT 양방향 순서 확인(요즘 잘 나가는 모델)

Language Model

확률모델

단어 시퀀스에 확률 부여

f(w1, w2, w3, ...wn) = prob

`단어 A` 다음에 `단어 B` 가 나올 확률을 계산할 수 있다 → 문장을 만들 수 있다

단어가 주로 어떤 단어와 주로 같이 나타나는지 살핀다

문맥에 의미가 녹아 있다

문장수준 임베딩: 요즘은 이게 대세

동음이의어를 분간할 수 있다 : 단어수준 임베딩에서는 불가능

`ELMo`, `BERT` 검색해보기

임베딩 활용

가장 크게 쓰일 수 있는 분야는 전이학습 Transfer Learning 다른 네트워크의 입력값으로 사용돼 자연어처리의 성능을 높일 수 있다

전이학습: 다른 딥러닝 모델의 입력값

전이학습 튜토리얼

# 단어 임베딩 구축

라이브러리 쓰지 않고 행렬연산으로

목표 : 단어 수준 임베딩 만들기

Deep Neural Networks 뉴럴렛

뉴럴렛을 Directed Acyclic Graph 로 볼 수 있다 :방향이 없고

확률 모델 

로지스틱 선형회귀처럼 0~1 사이의 확률값을 리턴 

그 안의 구조는 유연하게 설계할 수 있다

 input: 일반화된 벡터값

평가

학습 손실 loss: Cross Entropy

Backpropagation 역전파

딥러닝 학습 방법

Sum

Multiplication Node

Softmax + Cross entropy Node

최초의 방향벡터 구하기= 모델 예측값 - label

최초의 방향 * 로컬 그래디언트

loss 를 최소화하는 방향(그래디언트) 구하기

Word2Vec

Skip-Gram Model

타겟단어 t 

문맥단어 c : 타겟단어 앞뒤 각 2개 단어

타겟단어를 입력하면 문맥단어의 Prob 출력

학습이 끝나면 타겟단어를 바꿔서 계속

타겟 단어 d * |V|

문맥 단어 |V| * d 

`d`: 차원

`|V|`:  우리가 가지고 있는 어휘 수

24차원(계산량이 중요한 경우), 보통 128차원

품질 좋은 임베딩을 만들려면 말뭉치 GB 이상 차원은 100차원 이상

클로바에서 학습시키는데 쓰는 말뭉치: 블로그 카페 위키피디아 트위터

dL/dlogit 최초의 그래디언트 1*341(|V|)

### cosine similarity

unit vector 끼리의 내적dot product

cos similarity 가 높으면 관련성이 높다

유의, 반의는 구분해낼 수 없다

같은 문맥에서 쓰인다

Skip-Gram Model 문제점

softmax 확률을 구해야하는데 너무 오래걸린다

→ 대신 Sigmoid 를 써보자

    σ(x) = 1/1(+exp(-x)) # -1 <= σ(x) <= 1

타겟 단어를 넣어서 문맥단어를 맞추는 `multinomial classification` 을 풀었는데

타겟 단어-문맥단어 쌍이 실제 등장하는지 여부를 맞추는 `binary classificaion` 문제로 바꾼다

positive sample 은 cosine 유사도가 높아지게

negative sample 은 cosine 유사도가 낮아지게

임베딩으로 시각화 하는게 좋지는 않다(작위적이다)

고객에게 효과는 좋다